---
layout: post
title:  "Machine Learning"
date:   2019-04-14 20:46:47
categories: machinelearning code
---


I recently decided to figure out how neural networks work. Like everyone else, I started by watching 3b1b's video on neural nets, which you can find here. I then decided to build a neural network that takes in 4 numbers, either 0 or 1, and guesses one number, which can be either 0 or 1. (Yes, I can make a program that saves the results of all 16 different combinations but that's not the point). To do this, I used a neural net with 3 layers, the first with 4, the second with 4, and the third layer (the output) with 1 neuron. My code can be found [here](https://pastebin.com/EWC4VqCy).



## An Explanation


{% highlight java %}
 public static double forwards(int[] in) {
        input=in;
        for (int i = 0; i<4; i++){
          hidden[i]=0;
        }
        for (int i = 0; i<4; i++) {
            for (int j = 0; j<4; j++ ) {
                    hidden[j]+=layer1[i][j]*in[i];
            }
        }
       
        for (int i = 0 ; i<4; i++){
          hidden[i]=sigmoid(hidden[i]+bias[i]);
        }
        double ans = 0;
        for (int i = 0; i<4; i++) {
            ans+=hidden[i]*layer2[i];
        }
       
        return sigmoid(ans+bias2);
    }
    
{% endhighlight %}


This part of the code performs the forwards propagation part. It starts by calculating the values in the hidden layer by multiplying and adding the inputs (in) and the 1st layer weights (layer1). It then adds the bias and takes the sigmoid for each of the 4 values in the layer. 



It then calculates the second layer using a similar process, but using the layer 2 weights (stored in layer2) and the layer 2 bias (bias2). 


This part does backpropagation:

{% highlight java %}

 public static void backwards(double ans, double actual) {
      double learningR=1;
      bias2chg+=2*(actual-ans)*deriv(ans)/learningR;
     // System.out.println(ans);
        for (int i = 0; i<4; i++) {
            chg2[i]+=2*(actual-ans)*deriv(ans)*hidden[i]/learningR;
        }
        for (int i = 0; i<4; i++){
          biaschg[i]+=2*(actual-ans)*deriv(ans)*layer2[i]*deriv(hidden[i])/learningR;
        }
        for (int i = 0; i<4; i++) {
            for (int j = 0; j<4; j++) {
                chg1[i][j]+=2*(actual-ans)*deriv(ans)*layer2[j]*input[i]*deriv(hidden[j])/learningR;
            }
        }
       
    }


{% endhighlight %}


Yes, this code is very inefficient and I should use memoization instead. This will be fixed in my next attempt at making a neural net. For this step, I manually calculated all the derivatives (there weren't that many) and I just made my program calculate the derivatives using the formulas I found and change the weights and biases accordingly. There is also a variable which controls how big or small of a step the network takes every time.


{% highlight java %}

public static void train() {
      System.out.println("training");
        for (int i = 0; i<100000; i++) {
            int[] input = new int[4];
            for(int j = 0; j<4; j++) {
                input[j]=(int)(Math.round( Math.random() ));
            }
            double guess = forwards(input);
            double ans = function(input);
            backwards(guess, ans);
       
          bias2+=bias2chg;
          bias2chg=0;
          for (int l = 0; l<4; l++){
          bias[l]+=biaschg[l];
          biaschg[l]=0;
          layer2[l]+=chg2[l];
          chg2[l]=0;
          for (int m=0; m<4; m++){
            layer1[l][m]+=chg1[l][m];
            chg1[l][m]=0;
          }
          }}
        System.out.println("Done!");
        System.out.println("Stats:");
        System.out.println("\nAvg loss:");
        double total = 0;
        for (int i = 0; i<100; i++) {
            int[] input = new int[4];
            for(int j = 0; j<4; j++) {
                input[j]=(int)(Math.round( Math.random() ));
            }
             total+=getLoss(forwards(input), function(input));
        }
        System.out.println( total/100 );
        System.out.println("\nTest cases:\n");
        for (int i = 0; i<16; i++) {
            String x = Integer.toString( i, 2 );
            int[] input = new int[4];
            for (int j = 0; j<4; j++) {
                try {
                input[3-j]=Character.getNumericValue( x.charAt(x.length()-j-1));
                }
                catch(Exception e) {
                    input[3-j]=0;
                }
            }
            System.out.println( "Input: "+Arrays.toString( input) );
            System.out.println( "Guess: "+forwards(input) );
            System.out.println( "Actual: "+function(input) );
            System.out.println( "Loss: "+getLoss( forwards(input), function(input)) );
 
            System.out.println();
        }
    }


{% endhighlight %}



Finally, this part simply generates 100,000 random quadruples of numbers and runs forwards and backwards propagation over and over again. It then generates all 16 possible quadruples and compares the network's guess and the actual value.


<iframe height="400px" width="100%" src="https://repl.it/repls/DecimalFearlessGraphs?lite=true" scrolling="no" frameborder="no" allowtransparency="true" allowfullscreen="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals"></iframe>
